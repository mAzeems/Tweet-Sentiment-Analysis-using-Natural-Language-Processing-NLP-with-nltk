{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:27.136277Z","iopub.execute_input":"2022-06-20T06:49:27.136638Z","iopub.status.idle":"2022-06-20T06:49:32.706455Z","shell.execute_reply.started":"2022-06-20T06:49:27.136579Z","shell.execute_reply":"2022-06-20T06:49:32.705343Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def random_colours(number_of_colors):\n    '''\n    Simple function for random colours generation.\n    Input:\n        number_of_colors - integer value indicating the number of colours which are going to be generated.\n    Output:\n        Color in the following format: ['#E86DA4'] .\n    '''\n    colors = []\n    for i in range(number_of_colors):\n        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:32.708445Z","iopub.execute_input":"2022-06-20T06:49:32.708986Z","iopub.status.idle":"2022-06-20T06:49:32.722196Z","shell.execute_reply.started":"2022-06-20T06:49:32.708801Z","shell.execute_reply":"2022-06-20T06:49:32.721079Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Reading the Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nss = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:32.723727Z","iopub.execute_input":"2022-06-20T06:49:32.724351Z","iopub.status.idle":"2022-06-20T06:49:32.856799Z","shell.execute_reply.started":"2022-06-20T06:49:32.724297Z","shell.execute_reply":"2022-06-20T06:49:32.856021Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-20T06:49:32.858447Z","iopub.execute_input":"2022-06-20T06:49:32.859003Z","iopub.status.idle":"2022-06-20T06:49:32.864888Z","shell.execute_reply.started":"2022-06-20T06:49:32.858952Z","shell.execute_reply":"2022-06-20T06:49:32.863886Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"So We have 27486 tweets in the train set and 3535 tweets in the test set","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-20T06:49:32.869384Z","iopub.execute_input":"2022-06-20T06:49:32.869978Z","iopub.status.idle":"2022-06-20T06:49:32.895753Z","shell.execute_reply.started":"2022-06-20T06:49:32.869900Z","shell.execute_reply":"2022-06-20T06:49:32.895051Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We have one null Value in the train , as the test field for value is NAN we will just remove it","metadata":{}},{"cell_type":"code","source":"train.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:49:32.900318Z","iopub.execute_input":"2022-06-20T06:49:32.900604Z","iopub.status.idle":"2022-06-20T06:49:32.920137Z","shell.execute_reply.started":"2022-06-20T06:49:32.900560Z","shell.execute_reply":"2022-06-20T06:49:32.919215Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-20T06:49:32.921651Z","iopub.execute_input":"2022-06-20T06:49:32.922054Z","iopub.status.idle":"2022-06-20T06:49:32.932530Z","shell.execute_reply.started":"2022-06-20T06:49:32.921998Z","shell.execute_reply":"2022-06-20T06:49:32.931468Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"There are no null Values in the test set","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-20T06:49:32.934015Z","iopub.execute_input":"2022-06-20T06:49:32.934524Z","iopub.status.idle":"2022-06-20T06:49:32.956004Z","shell.execute_reply.started":"2022-06-20T06:49:32.934352Z","shell.execute_reply":"2022-06-20T06:49:32.955162Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Selected_text is a subset of text ","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-20T06:49:32.957420Z","iopub.execute_input":"2022-06-20T06:49:32.958004Z","iopub.status.idle":"2022-06-20T06:49:33.060069Z","shell.execute_reply.started":"2022-06-20T06:49:32.957956Z","shell.execute_reply":"2022-06-20T06:49:33.059047Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"temp = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:33.061616Z","iopub.execute_input":"2022-06-20T06:49:33.062063Z","iopub.status.idle":"2022-06-20T06:49:33.173485Z","shell.execute_reply.started":"2022-06-20T06:49:33.062010Z","shell.execute_reply":"2022-06-20T06:49:33.172393Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(x='sentiment',data=train)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:33.175033Z","iopub.execute_input":"2022-06-20T06:49:33.175561Z","iopub.status.idle":"2022-06-20T06:49:33.416297Z","shell.execute_reply.started":"2022-06-20T06:49:33.175384Z","shell.execute_reply":"2022-06-20T06:49:33.415278Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure(go.Funnelarea(\n    text =temp.sentiment,\n    values = temp.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:33.417669Z","iopub.execute_input":"2022-06-20T06:49:33.418203Z","iopub.status.idle":"2022-06-20T06:49:34.765862Z","shell.execute_reply.started":"2022-06-20T06:49:33.418152Z","shell.execute_reply":"2022-06-20T06:49:34.764989Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:34.767317Z","iopub.execute_input":"2022-06-20T06:49:34.767826Z","iopub.status.idle":"2022-06-20T06:49:34.773862Z","shell.execute_reply.started":"2022-06-20T06:49:34.767652Z","shell.execute_reply":"2022-06-20T06:49:34.772590Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"results_jaccard=[]\n\nfor ind,row in train.iterrows():\n    sentence1 = row.text\n    sentence2 = row.selected_text\n\n    jaccard_score = jaccard(sentence1,sentence2)\n    results_jaccard.append([sentence1,sentence2,jaccard_score])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:34.775931Z","iopub.execute_input":"2022-06-20T06:49:34.776796Z","iopub.status.idle":"2022-06-20T06:49:38.462876Z","shell.execute_reply.started":"2022-06-20T06:49:34.776422Z","shell.execute_reply":"2022-06-20T06:49:38.461914Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"jaccard = pd.DataFrame(results_jaccard,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\ntrain = train.merge(jaccard,how='outer')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:38.464304Z","iopub.execute_input":"2022-06-20T06:49:38.464670Z","iopub.status.idle":"2022-06-20T06:49:38.512448Z","shell.execute_reply.started":"2022-06-20T06:49:38.464612Z","shell.execute_reply":"2022-06-20T06:49:38.511660Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train['Num_words_ST'] = train['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ntrain['Num_word_text'] = train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ntrain['difference_in_words'] = train['Num_word_text'] - train['Num_words_ST'] #Difference in Number of words text and Selected Text","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:38.514083Z","iopub.execute_input":"2022-06-20T06:49:38.514638Z","iopub.status.idle":"2022-06-20T06:49:38.713815Z","shell.execute_reply.started":"2022-06-20T06:49:38.514519Z","shell.execute_reply":"2022-06-20T06:49:38.712961Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:49:38.715312Z","iopub.execute_input":"2022-06-20T06:49:38.715683Z","iopub.status.idle":"2022-06-20T06:49:38.731117Z","shell.execute_reply.started":"2022-06-20T06:49:38.715623Z","shell.execute_reply":"2022-06-20T06:49:38.730040Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the distribution of Meta-Features","metadata":{}},{"cell_type":"code","source":"hist_data = [train['Num_words_ST'],train['Num_word_text']]\n\ngroup_labels = ['Selected_Text', 'Text']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels,show_curve=False)\nfig.update_layout(title_text='Distribution of Number Of words')\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=700,\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:38.732883Z","iopub.execute_input":"2022-06-20T06:49:38.733529Z","iopub.status.idle":"2022-06-20T06:49:40.535373Z","shell.execute_reply.started":"2022-06-20T06:49:38.733477Z","shell.execute_reply":"2022-06-20T06:49:40.534276Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"* The number of words plot is really interesting ,the tweets having number of words greater than 25 are very less and thus the number of words distribution plot is right skewed","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train['Num_words_ST'], shade=True, color=\"r\").set_title('Kernel Distribution of Number Of words')\np1=sns.kdeplot(train['Num_word_text'], shade=True, color=\"b\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:49:40.536697Z","iopub.execute_input":"2022-06-20T06:49:40.537066Z","iopub.status.idle":"2022-06-20T06:49:40.864869Z","shell.execute_reply.started":"2022-06-20T06:49:40.537023Z","shell.execute_reply":"2022-06-20T06:49:40.863966Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**Now It will be more interesting to see the differnce in number of words and jaccard_scores across different Sentiments**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['difference_in_words'], shade=True, color=\"b\").set_title('Kernel Distribution of Difference in Number Of words')\np2=sns.kdeplot(train[train['sentiment']=='negative']['difference_in_words'], shade=True, color=\"r\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:40.866274Z","iopub.execute_input":"2022-06-20T06:49:40.866783Z","iopub.status.idle":"2022-06-20T06:49:41.201541Z","shell.execute_reply.started":"2022-06-20T06:49:40.866730Z","shell.execute_reply":"2022-06-20T06:49:41.200556Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['difference_in_words'],kde=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:49:41.202974Z","iopub.execute_input":"2022-06-20T06:49:41.203504Z","iopub.status.idle":"2022-06-20T06:49:41.564074Z","shell.execute_reply.started":"2022-06-20T06:49:41.203455Z","shell.execute_reply":"2022-06-20T06:49:41.562860Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"I was not able to plot kde plot for neutral tweets because most of the values for difference in number of words were zero. We can see it clearly now ,if we had used the feature in the starting we would have known that text and selected text are mostly the same for neutral tweets,thus its always important to keep the end goal in mind while performing EDA","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['jaccard_score'], shade=True, color=\"b\").set_title('KDE of Jaccard Scores across different Sentiments')\np2=sns.kdeplot(train[train['sentiment']=='negative']['jaccard_score'], shade=True, color=\"r\")\nplt.legend(labels=['positive','negative'])","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:49:41.565749Z","iopub.execute_input":"2022-06-20T06:49:41.566131Z","iopub.status.idle":"2022-06-20T06:49:41.922944Z","shell.execute_reply.started":"2022-06-20T06:49:41.566083Z","shell.execute_reply":"2022-06-20T06:49:41.921756Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"I was not able to plot kde of jaccard_scores of neutral tweets for the same reason,thus I will plot a distribution plot","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['jaccard_score'],kde=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:49:41.924621Z","iopub.execute_input":"2022-06-20T06:49:41.925005Z","iopub.status.idle":"2022-06-20T06:49:42.282810Z","shell.execute_reply.started":"2022-06-20T06:49:41.924956Z","shell.execute_reply":"2022-06-20T06:49:42.281659Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"k = train[train['Num_word_text']<=2]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:49:42.284447Z","iopub.execute_input":"2022-06-20T06:49:42.285062Z","iopub.status.idle":"2022-06-20T06:49:42.293753Z","shell.execute_reply.started":"2022-06-20T06:49:42.285009Z","shell.execute_reply":"2022-06-20T06:49:42.292404Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"k.groupby('sentiment').mean()['jaccard_score']","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:49:42.299136Z","iopub.execute_input":"2022-06-20T06:49:42.301709Z","iopub.status.idle":"2022-06-20T06:49:42.318115Z","shell.execute_reply.started":"2022-06-20T06:49:42.301648Z","shell.execute_reply":"2022-06-20T06:49:42.317252Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"k[k['sentiment']=='positive']","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:49:42.319591Z","iopub.execute_input":"2022-06-20T06:49:42.320220Z","iopub.status.idle":"2022-06-20T06:49:42.350349Z","shell.execute_reply.started":"2022-06-20T06:49:42.320170Z","shell.execute_reply":"2022-06-20T06:49:42.349329Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:49:42.351933Z","iopub.execute_input":"2022-06-20T06:49:42.352650Z","iopub.status.idle":"2022-06-20T06:49:42.360644Z","shell.execute_reply.started":"2022-06-20T06:49:42.352596Z","shell.execute_reply":"2022-06-20T06:49:42.359664Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:49:42.362626Z","iopub.execute_input":"2022-06-20T06:49:42.363184Z","iopub.status.idle":"2022-06-20T06:49:44.249076Z","shell.execute_reply.started":"2022-06-20T06:49:42.363101Z","shell.execute_reply":"2022-06-20T06:49:44.248239Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:49:44.250447Z","iopub.execute_input":"2022-06-20T06:49:44.250833Z","iopub.status.idle":"2022-06-20T06:49:44.265528Z","shell.execute_reply.started":"2022-06-20T06:49:44.250787Z","shell.execute_reply":"2022-06-20T06:49:44.264590Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Most Common words in our Target-Selected Text","metadata":{}},{"cell_type":"code","source":"train['temp_list'] = train['selected_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:44.266849Z","iopub.execute_input":"2022-06-20T06:49:44.267487Z","iopub.status.idle":"2022-06-20T06:49:44.372771Z","shell.execute_reply.started":"2022-06-20T06:49:44.267430Z","shell.execute_reply":"2022-06-20T06:49:44.371863Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:44.374174Z","iopub.execute_input":"2022-06-20T06:49:44.374696Z","iopub.status.idle":"2022-06-20T06:49:45.004321Z","shell.execute_reply.started":"2022-06-20T06:49:44.374644Z","shell.execute_reply":"2022-06-20T06:49:45.003393Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def remove_stopword(x):\n    return [y for y in x if y not in stopwords.words('english')]\ntrain['temp_list'] = train['temp_list'].apply(lambda x:remove_stopword(x))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:49:45.011308Z","iopub.execute_input":"2022-06-20T06:49:45.011625Z","iopub.status.idle":"2022-06-20T06:50:09.669730Z","shell.execute_reply.started":"2022-06-20T06:49:45.011578Z","shell.execute_reply":"2022-06-20T06:50:09.668756Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"top = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:09.671352Z","iopub.execute_input":"2022-06-20T06:50:09.671717Z","iopub.status.idle":"2022-06-20T06:50:09.723210Z","shell.execute_reply.started":"2022-06-20T06:50:09.671669Z","shell.execute_reply":"2022-06-20T06:50:09.722063Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:09.725374Z","iopub.execute_input":"2022-06-20T06:50:09.726973Z","iopub.status.idle":"2022-06-20T06:50:10.075946Z","shell.execute_reply.started":"2022-06-20T06:50:09.725782Z","shell.execute_reply":"2022-06-20T06:50:10.074955Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Most Common words in Text\n\nLet's also look at the most common words in Text","metadata":{}},{"cell_type":"code","source":"train['temp_list1'] = train['text'].apply(lambda x:str(x).split()) #List of words in every row for text\ntrain['temp_list1'] = train['temp_list1'].apply(lambda x:remove_stopword(x)) #Removing Stopwords","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:10.077356Z","iopub.execute_input":"2022-06-20T06:50:10.077706Z","iopub.status.idle":"2022-06-20T06:50:54.028787Z","shell.execute_reply.started":"2022-06-20T06:50:10.077659Z","shell.execute_reply":"2022-06-20T06:50:54.027995Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"top = Counter([item for sublist in train['temp_list1'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:54.030302Z","iopub.execute_input":"2022-06-20T06:50:54.030699Z","iopub.status.idle":"2022-06-20T06:50:54.114645Z","shell.execute_reply.started":"2022-06-20T06:50:54.030652Z","shell.execute_reply":"2022-06-20T06:50:54.113798Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"So the first two common word was I'm so I removed it and took data from second row","metadata":{}},{"cell_type":"code","source":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:54.116317Z","iopub.execute_input":"2022-06-20T06:50:54.116711Z","iopub.status.idle":"2022-06-20T06:50:54.632773Z","shell.execute_reply.started":"2022-06-20T06:50:54.116663Z","shell.execute_reply":"2022-06-20T06:50:54.632055Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"SO we can see the Most common words in Selected text and Text are almost the same,which was obvious","metadata":{}},{"cell_type":"markdown","source":"# Most common words Sentiments Wise\n\nLet's look at the most common words in different sentiments","metadata":{}},{"cell_type":"code","source":"Positive_sent = train[train['sentiment']=='positive']\nNegative_sent = train[train['sentiment']=='negative']\nNeutral_sent = train[train['sentiment']=='neutral']","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:50:54.634313Z","iopub.execute_input":"2022-06-20T06:50:54.634707Z","iopub.status.idle":"2022-06-20T06:50:54.663637Z","shell.execute_reply.started":"2022-06-20T06:50:54.634639Z","shell.execute_reply":"2022-06-20T06:50:54.662958Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['temp_list'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:54.664856Z","iopub.execute_input":"2022-06-20T06:50:54.665273Z","iopub.status.idle":"2022-06-20T06:50:54.699212Z","shell.execute_reply.started":"2022-06-20T06:50:54.665226Z","shell.execute_reply":"2022-06-20T06:50:54.698253Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Positive Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:54.700933Z","iopub.execute_input":"2022-06-20T06:50:54.701949Z","iopub.status.idle":"2022-06-20T06:50:55.352043Z","shell.execute_reply.started":"2022-06-20T06:50:54.701873Z","shell.execute_reply":"2022-06-20T06:50:55.351147Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['temp_list'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:55.353571Z","iopub.execute_input":"2022-06-20T06:50:55.354211Z","iopub.status.idle":"2022-06-20T06:50:55.390006Z","shell.execute_reply.started":"2022-06-20T06:50:55.354159Z","shell.execute_reply":"2022-06-20T06:50:55.389055Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"fig = px.treemap(temp_negative, path=['Common_words'], values='count',title='Tree Of Most Common Negative Words')\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:55.391560Z","iopub.execute_input":"2022-06-20T06:50:55.392210Z","iopub.status.idle":"2022-06-20T06:50:55.709684Z","shell.execute_reply.started":"2022-06-20T06:50:55.392158Z","shell.execute_reply":"2022-06-20T06:50:55.708983Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"#MosT common Neutral words\ntop = Counter([item for sublist in Neutral_sent['temp_list'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral.style.background_gradient(cmap='Reds')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:55.711846Z","iopub.execute_input":"2022-06-20T06:50:55.712476Z","iopub.status.idle":"2022-06-20T06:50:55.755397Z","shell.execute_reply.started":"2022-06-20T06:50:55.712423Z","shell.execute_reply":"2022-06-20T06:50:55.754431Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(temp_neutral, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:55.756800Z","iopub.execute_input":"2022-06-20T06:50:55.757349Z","iopub.status.idle":"2022-06-20T06:50:56.227626Z","shell.execute_reply.started":"2022-06-20T06:50:55.757298Z","shell.execute_reply":"2022-06-20T06:50:56.226895Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"fig = px.treemap(temp_neutral, path=['Common_words'], values='count',title='Tree Of Most Common Neutral Words')\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:56.228920Z","iopub.execute_input":"2022-06-20T06:50:56.229296Z","iopub.status.idle":"2022-06-20T06:50:56.546832Z","shell.execute_reply.started":"2022-06-20T06:50:56.229248Z","shell.execute_reply":"2022-06-20T06:50:56.545899Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"raw_text = [word for word_list in train['temp_list1'] for word in word_list]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:50:56.548324Z","iopub.execute_input":"2022-06-20T06:50:56.548947Z","iopub.status.idle":"2022-06-20T06:50:56.569761Z","shell.execute_reply.started":"2022-06-20T06:50:56.548797Z","shell.execute_reply":"2022-06-20T06:50:56.569037Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def words_unique(sentiment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'Neutral');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train[train.sentiment != sentiment]['temp_list1']:\n        for word in item:\n            allother .append(word)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train[train.sentiment == sentiment]['temp_list1']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:50:56.571514Z","iopub.execute_input":"2022-06-20T06:50:56.572061Z","iopub.status.idle":"2022-06-20T06:50:56.580910Z","shell.execute_reply.started":"2022-06-20T06:50:56.571883Z","shell.execute_reply":"2022-06-20T06:50:56.580024Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### Positive Tweets","metadata":{}},{"cell_type":"code","source":"Unique_Positive= words_unique('positive', 20, raw_text)\nprint(\"The top 20 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:50:56.582327Z","iopub.execute_input":"2022-06-20T06:50:56.582945Z","iopub.status.idle":"2022-06-20T06:51:56.738603Z","shell.execute_reply.started":"2022-06-20T06:50:56.582877Z","shell.execute_reply":"2022-06-20T06:51:56.737700Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"fig = px.treemap(Unique_Positive, path=['words'], values='count',title='Tree Of Unique Positive Words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:51:56.740310Z","iopub.execute_input":"2022-06-20T06:51:56.740965Z","iopub.status.idle":"2022-06-20T06:51:57.491789Z","shell.execute_reply.started":"2022-06-20T06:51:56.740718Z","shell.execute_reply":"2022-06-20T06:51:57.490916Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Positive['count'], labels=Unique_Positive.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Positive Words')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:51:57.496290Z","iopub.execute_input":"2022-06-20T06:51:57.498526Z","iopub.status.idle":"2022-06-20T06:51:57.883812Z","shell.execute_reply.started":"2022-06-20T06:51:57.498472Z","shell.execute_reply":"2022-06-20T06:51:57.882809Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"Unique_Negative= words_unique('negative', 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Reds')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:51:57.885206Z","iopub.execute_input":"2022-06-20T06:51:57.885861Z","iopub.status.idle":"2022-06-20T06:52:59.136546Z","shell.execute_reply.started":"2022-06-20T06:51:57.885806Z","shell.execute_reply":"2022-06-20T06:52:59.135607Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.rcParams['text.color'] = 'black'\nplt.pie(Unique_Negative['count'], labels=Unique_Negative.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Negative Words')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:52:59.138000Z","iopub.execute_input":"2022-06-20T06:52:59.138375Z","iopub.status.idle":"2022-06-20T06:52:59.378358Z","shell.execute_reply.started":"2022-06-20T06:52:59.138326Z","shell.execute_reply":"2022-06-20T06:52:59.377223Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"Unique_Neutral= words_unique('neutral', 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Oranges')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:52:59.383906Z","iopub.execute_input":"2022-06-20T06:52:59.384465Z","iopub.status.idle":"2022-06-20T06:53:55.586146Z","shell.execute_reply.started":"2022-06-20T06:52:59.384302Z","shell.execute_reply":"2022-06-20T06:53:55.585410Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Neutral['count'], labels=Unique_Neutral.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Neutral Words')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T06:53:55.587597Z","iopub.execute_input":"2022-06-20T06:53:55.588000Z","iopub.status.idle":"2022-06-20T06:53:55.814833Z","shell.execute_reply.started":"2022-06-20T06:53:55.587949Z","shell.execute_reply":"2022-06-20T06:53:55.813899Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"def clean_text(data):\n    # convert catacter to lowercase\n    data['clean_text']=data['text'].str.lower()\n    #remove URLS\n    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r\"http\\S+\", \"\", elem))\n    #remove ponctuation\n    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r\"[^\\w\\s]\", \"\", elem))\n    #remove \n    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r'/n',\"\",elem))\n    #remove degits\n    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r'\\d+',\"\",elem))\n    #remove multiple spaces\n    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r'\\s+',\" \",elem))\n    #remove single caracter\n    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r'\\s+[a-zA-Z]\\s+',\" \",elem))\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:53:55.816439Z","iopub.execute_input":"2022-06-20T06:53:55.817107Z","iopub.status.idle":"2022-06-20T06:53:55.829802Z","shell.execute_reply.started":"2022-06-20T06:53:55.817045Z","shell.execute_reply":"2022-06-20T06:53:55.828816Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"train=clean_text(train)\n#data_test=clean_text(data_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:53:55.831606Z","iopub.execute_input":"2022-06-20T06:53:55.832279Z","iopub.status.idle":"2022-06-20T06:53:56.495915Z","shell.execute_reply.started":"2022-06-20T06:53:55.832226Z","shell.execute_reply":"2022-06-20T06:53:56.495077Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def remove_Stopwords(text):\n    stopW=stopwords.words('english') #get the english stopwords\n    return \" \".join([i for i in text.split() if i not in stopW])\n\ntrain['clean_text']=train['clean_text'].apply(lambda x:remove_Stopwords(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:53:56.497414Z","iopub.execute_input":"2022-06-20T06:53:56.497794Z","iopub.status.idle":"2022-06-20T06:54:00.859421Z","shell.execute_reply.started":"2022-06-20T06:53:56.497746Z","shell.execute_reply":"2022-06-20T06:54:00.858594Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nfrom nltk.corpus import stopwords,wordnet\nimport nltk \nfrom nltk.stem import WordNetLemmatizer\ndef get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN)\n\ndef lemmatize(text):\n    # 1. Init Lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    # 2. Lemmatize text with the appropriate POS tag\n    return \" \".join([lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in text.split()])\n\n\n#Apply lemmatizer to each row in the dataframe\ntrain['clean_text'] = train['clean_text'].apply(lambda x:lemmatize(x) )\n#data_test['clean_text'] = data_test['clean_text'].apply(lambda x:lemmatize(x) )","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:54:00.860976Z","iopub.execute_input":"2022-06-20T06:54:00.861400Z","iopub.status.idle":"2022-06-20T06:54:38.758044Z","shell.execute_reply.started":"2022-06-20T06:54:00.861348Z","shell.execute_reply":"2022-06-20T06:54:38.757191Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"#display exemple of data before and after cleaning\nprint(train['text'][2])\nprint(train['clean_text'][2])","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:54:38.759408Z","iopub.execute_input":"2022-06-20T06:54:38.759830Z","iopub.status.idle":"2022-06-20T06:54:38.768025Z","shell.execute_reply.started":"2022-06-20T06:54:38.759776Z","shell.execute_reply":"2022-06-20T06:54:38.767030Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nmax_words = 5000\nmax_len=50\n\ndef tokenize_pad_sequences(text):\n    '''\n    This function tokenize the input text into sequnences of intergers and then\n    pad each sequence to the same length\n    '''\n    # Text tokenization\n    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n    tokenizer.fit_on_texts(text)\n    # Transforms text to a sequence of integers\n    X = tokenizer.texts_to_sequences(text)\n    # Pad sequences to the same length\n    X = pad_sequences(X, padding='post', maxlen=max_len)\n    # return sequences\n    return X, tokenizer\n\nprint('Before Tokenization & Padding \\n', train['clean_text'][0])\nX, tokenizer = tokenize_pad_sequences(train['clean_text'])\nprint('After Tokenization & Padding \\n', X[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:54:38.769705Z","iopub.execute_input":"2022-06-20T06:54:38.770324Z","iopub.status.idle":"2022-06-20T06:54:44.058446Z","shell.execute_reply.started":"2022-06-20T06:54:38.770273Z","shell.execute_reply":"2022-06-20T06:54:44.057582Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# saving\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n# loading\nwith open('tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:54:44.060994Z","iopub.execute_input":"2022-06-20T06:54:44.061666Z","iopub.status.idle":"2022-06-20T06:54:44.112720Z","shell.execute_reply.started":"2022-06-20T06:54:44.061598Z","shell.execute_reply":"2022-06-20T06:54:44.111882Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny = pd.get_dummies(train['sentiment'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\nprint('Train Set ->', X_train.shape, y_train.shape)\nprint('Validation Set ->', X_val.shape, y_val.shape)\nprint('Test Set ->', X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:54:44.114112Z","iopub.execute_input":"2022-06-20T06:54:44.114472Z","iopub.status.idle":"2022-06-20T06:54:44.141111Z","shell.execute_reply.started":"2022-06-20T06:54:44.114421Z","shell.execute_reply":"2022-06-20T06:54:44.140405Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"import keras.backend as K\n\ndef f1_score(precision, recall):\n    ''' Function to calculate f1 score '''\n    \n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:54:44.142593Z","iopub.execute_input":"2022-06-20T06:54:44.143037Z","iopub.status.idle":"2022-06-20T06:54:44.148379Z","shell.execute_reply.started":"2022-06-20T06:54:44.142981Z","shell.execute_reply":"2022-06-20T06:54:44.147525Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\nfrom keras.metrics import Precision, Recall\nfrom keras.optimizers import SGD\nfrom keras.optimizers import RMSprop\nfrom keras import datasets\n\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.callbacks import History\n\nfrom keras import losses\n\nvocab_size = 5000\nembedding_size = 32\nepochs=25\nlearning_rate = 0.1\ndecay_rate = learning_rate / epochs\nmomentum = 0.8\n\nsgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n# Build model\nmodel= Sequential()\nmodel.add(Embedding(vocab_size, embedding_size, input_length=max_len))\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(3, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:06:13.263435Z","iopub.execute_input":"2022-06-20T07:06:13.263796Z","iopub.status.idle":"2022-06-20T07:06:13.513645Z","shell.execute_reply.started":"2022-06-20T07:06:13.263744Z","shell.execute_reply":"2022-06-20T07:06:13.512723Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:06:15.442336Z","iopub.execute_input":"2022-06-20T07:06:15.442781Z","iopub.status.idle":"2022-06-20T07:06:15.608251Z","shell.execute_reply.started":"2022-06-20T07:06:15.442723Z","shell.execute_reply":"2022-06-20T07:06:15.607035Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"print(model.summary())\n\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, \n               metrics=['accuracy', Precision(), Recall()])\n\n# Train model\n\nbatch_size = 64\nhistory = model.fit(X_train, y_train,\n                      validation_data=(X_val, y_val),\n                      batch_size=batch_size, epochs=epochs, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:06:17.884859Z","iopub.execute_input":"2022-06-20T07:06:17.885284Z","iopub.status.idle":"2022-06-20T07:11:29.664894Z","shell.execute_reply.started":"2022-06-20T07:06:17.885222Z","shell.execute_reply":"2022-06-20T07:11:29.664030Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# Evaluate model on the test set\nloss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n# Print metrics\nprint('')\nprint('Accuracy  : {:.4f}'.format(accuracy))\nprint('Precision : {:.4f}'.format(precision))\nprint('Recall    : {:.4f}'.format(recall))\nprint('F1 Score  : {:.4f}'.format(f1_score(precision, recall)))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:12:19.969695Z","iopub.execute_input":"2022-06-20T07:12:19.970107Z","iopub.status.idle":"2022-06-20T07:12:21.424628Z","shell.execute_reply.started":"2022-06-20T07:12:19.970048Z","shell.execute_reply":"2022-06-20T07:12:21.423684Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"def plot_training_hist(history):\n    '''Function to plot history for accuracy and loss'''\n    \n    fig, ax = plt.subplots(1, 2, figsize=(10,4))\n    # first plot\n    ax[0].plot(history.history['accuracy'])\n    ax[0].plot(history.history['val_accuracy'])\n    ax[0].set_title('Model Accuracy')\n    ax[0].set_xlabel('epoch')\n    ax[0].set_ylabel('accuracy')\n    ax[0].legend(['train', 'validation'], loc='best')\n    # second plot\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('Model Loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].set_ylabel('loss')\n    ax[1].legend(['train', 'validation'], loc='best')\n    \nplot_training_hist(history)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:12:23.758066Z","iopub.execute_input":"2022-06-20T07:12:23.758478Z","iopub.status.idle":"2022-06-20T07:12:24.532570Z","shell.execute_reply.started":"2022-06-20T07:12:23.758424Z","shell.execute_reply":"2022-06-20T07:12:24.531560Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(model, X_test, y_test):\n    '''Function to plot confusion matrix for the passed model and the data'''\n    \n    sentiment_classes = ['Negative', 'Neutral', 'Positive']\n    # use model to do the prediction\n    y_pred = model.predict(X_test)\n    # compute confusion matrix\n    cm = confusion_matrix(np.argmax(np.array(y_test),axis=1), np.argmax(y_pred, axis=1))\n    # plot confusion matrix\n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', \n                xticklabels=sentiment_classes,\n                yticklabels=sentiment_classes)\n    plt.title('Confusion matrix', fontsize=16)\n    plt.xlabel('Actual label', fontsize=12)\n    plt.ylabel('Predicted label', fontsize=12)\n    \nplot_confusion_matrix(model, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:12:26.827365Z","iopub.execute_input":"2022-06-20T07:12:26.827738Z","iopub.status.idle":"2022-06-20T07:12:28.656409Z","shell.execute_reply.started":"2022-06-20T07:12:26.827686Z","shell.execute_reply":"2022-06-20T07:12:28.655294Z"},"trusted":true},"execution_count":75,"outputs":[]}]}